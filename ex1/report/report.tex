% !TeX spellcheck = en_US
\documentclass[sigconf,nonacm]{acmart}
\settopmatter{printacmref=false}
\pagestyle{empty}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\copyrightyear{2020}
\acmYear{2020}
\setcopyright{rightsretained}
\begin{document}
\title{Security, Privacy \& Explainability in Machine Learning}
\subtitle{Exercise 1: Record-Linkage Attack}
\author{Thomas Jirout}
\email{thomas.jirout@tuwien.ac.at}
\affiliation{Mat.Nr. 01525606}
\author{Helmuth Breitenfellner}
\email{helmuth.breitenfellner@student.tuwien.ac.at}
\affiliation{Mat.Nr. 08725866}
\begin{abstract}
In this task we have been working on record-linkage attack.
For this we chose the dataset DBLP-Scholar provided by the University of Leipzig \cite{DataSets},\cite{kopcke2010evaluation}
and used the Python Record Linkage Toolkit \cite{de_bruin_j_2019_3559043}.

This report summarizes the identification of sensitive attributes, attributes
used for linking, and the performance of the record-linkage attack.
\end{abstract}
\keywords{Security, Privacy, Machine Learning, Record Linkage, Python}
\maketitle
\section{Task description and Fundamentals}

When performing a record linkage attack, there are a couple of steps
to be done:
\begin{itemize}
\item Acquire datasets
\item Cleanup \& pre-process data
\item Identify candidate pairs for linkage
\item Calculate matches on field-levels
\item Classify linked records
\item Evaluate classification
\end{itemize}

\subsection{Acquire datasets}

We were using the evaluation datasets, provided by the University Leipzig.
These datasets are already harmonized, i.e. the columns have the same names,
and there is also a set of ``ground truth'' linkage information
which we were using for the evaluation.

\subsection{Cleanup \& pre-processing}

Datasets contain errors: wrongly encoded strings, typos, or
incorrect years.
The purpose of the cleanup step is to reduce the influence of these
errors.
Approaches available include:
\begin{itemize}
\item Fuzzy text encoding, e.g. ``soundex'' code
\item Harmonization, e.g. lowercase of all text, removal of non-text characters
\end{itemize}

\subsection{Identify candidate pairs}

For performing a record linkage attack, one would have to compare each
record of the first with each record of the second dataset.
This becomes quickly infeasible once datasets reach hundred thousands
of records.

One approach to reduce this effort is using ``blocking''.
Here attributes are looked for which \emph{have} to agree for a
match to be considered.
Possible choice could be years where the errors are assumed to be
less prominent.

A variant of blocking is using a neighborhood. Instead of only
considering candidates with the same value one allows a small
difference of the values between the datasets.

Both blocking and neighborhoods significantly reduce the number
of candidate pairs to be inspected.

\subsection{Calculate matches on field-level}

For comparing strings there are multiple methods to gather a measure
of similarity supported by the Record Linkage Toolkit.
\begin{description}
\item[Levenshtein:]
The Levenshtein distance \cite{Levenshtein}
represents the number of edits required to change
from one to the other string.
\item[Damerau-Levenshtein:]
This is an enhancement suggested by Damerau \cite{Damerau} which
added an operation (transpose) to the set of possible edits.
\item[Jaro:]
The Jaro distance \cite{Jaro} gives a floating point value
in the interval [0,1] where 0 represents complete dissimilarity and 1
represents identical strings.
\item[Jaro-Winkler:]
This is an improvement on the Jaro distance described by Winker \cite{Winkler}
which also favors matches of longer common prefixes.
\item[Q-gram:]
This is an approximation for the Damerau-Levenshtein distance
described by Ukkonen \cite{Ukkonen}
which is quicker to calculate.
\item[Cosine:]
This is the usual cosine similarity, applied on the characters of
the strings.
\item[Smith-Waterman:]
This algorithm \cite{Smith-Waterman}, originally designed for finding similarities in
chemical molecules, looks for long identical subsequences to identify the similarity
of two strings.
\end{description}

An alternative to using these algorithms is using a phonetic code in
the pre-processing like soundex and then only allowing exact matches.

\subsection{Classify linked records}

To find the matches is a classification problem: given the calculated features
of field similarity, is this pair a real match?

There are multiple approaches implemented in the recordlinkage toolkit.

Some of them are supervised approaches, like \emph{Logistic Regression},
\emph{Naive Bayes}, \emph{Support Vector Machine}.
These approaches need a training phase to get the best parameters for
the classification.

Others are unsupervised, like \emph{KMeans Clustering} or
\emph{Expectation / Conditional Maximization}.

We created a \emph{train} dataset of about 20\% of the data to train
the supervised algorithms.
For comparability all algorithms (including the unsupervised ones)
were evaluated on the disjoint \emph{test} dataset.

\subsection{Evaluate}

There are multiple metrics for evaluating the quality of the classification.

We opted for metrics which do not count the number of pairs, as we felt
this is limiting in the results.
Therefore we opted for using the F1 score when looking at the quality
of the record linkage.

\section{Datasets}

For our record linkage experiments, we used the DBLP and Gogle Scholar evaluation datasets, provided by the University of Leipzig \cite{DataSets},\cite{kopcke2010evaluation}:

\begin{description}
\item[DBLP:] Publication records from the Digital Bibliography \& Library Project, consisting of 2616 records.  
\item[Scholar:] Publication records from Google Scholar, consisting of 64263 records.
\item[Perfect Mapping:] Along with the DBLP and Scholar datasets, the University of Leipzig also provided a ``perfect mapping'', which we used in our evaluation step to determine the quality of our record linkage results. According to this perfect mapping, the best-possible linkage would be able to find 5348 id-to-id matches. It is worth noting that it can be seen from this document that assignments are m-to-n, meaning that it is possible that any entry in one dataset may correspond to multiple entries in the other one.
\end{description}

These datasets were already harmonized, i.e. the columns had the same names: id (string), title of the publication (string), authors (string), publication venue (string or NULL) and year (integer or NULL).

\subsection{Data Linking}

Due to the nature of the ready-prepared dataset, the identification of the
linked attributes was very easy.

We used the following attributes to link records:
\textbf{Name} (of the publication), \textbf{Venue} and \textbf{Authors}.

\subsection{Example Data}

As a demonstration of the difficulty of matching two entries, Table \ref{ExampleData} shows rows that would be a match, but are represented in a slightly different manner in each of the datasets.

\begin{table*}[t]
	\begin{tabular}{|l|l|l|l|l|l|}
	\hline
	dataset & id & title                        & authors                                                                                                                                                                              & venue             & year \\ \hline
	DBLP    &  journals/sigmod/SnodgrassAABC  & TSQL2 Language Specification & R Snodgrass, I Ahn, [...] & SIGMOD Record     & 1994 \\ \hline
	Scholar &   64iWa-ZiPRcJ & TSQL2 language specification & RT Snodgrass                                                                                                                                                                         & ACM SIGMOD Record & 1994 \\ \hline
	\end{tabular}
	\caption{Example Data Entries from DBLP and Scholar Dataset (id and authors column abbreviated)}
	\label{ExampleData}
\end{table*}

\subsection{Sensitive attributes}

Normally datasets of articles do not contain sensitive information,
and authors would like to have the contained data shared with
as many people as possible.

For the purpose of out evaluation we were qualifying the
ID on Google Scholar as sensitive.

The idea is that through these ID values more information
can potentially be revealed, e.g. citations of the authors.


\section{Evaluation \& Summary}

For the evaluation we looked at the \emph{F1-score} as a unified
measure of both recall and precision.

\subsection{Impact of Indexing Step}



\subsection{Impact of comparison methodology}

\subsection{Impact of used Classifier}


\bibliographystyle{ACM-Reference-Format}
\bibliography{report}
\end{document}
