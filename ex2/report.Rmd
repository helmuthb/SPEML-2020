---
title: "Security, Privacy & Explainability in Machine Learning"
author:
- name: Thomas Jirout
  affiliation: Mat.Nr. 01525606
  email: thomas.jirout\@tuwien.ac.at
- name: Helmuth Breitenfellner
  affiliation: Mat.Nr. 08725866
  email: helmuth.breitenfellner\@student.tuwien.ac.at
setcopyright: rightsretained
output:
  pdf_document:
    template: report_template.tex
    keep_tex: yes
  html_document:
    df_print: paged
acm-year: 2020
subtitle: 'Exercise 2: Explainability - Exploring a Back-Door''ed Model'
keywords: Explainability, Backdoor, Machine Learning, ALE, ICE
copyright-year: 2020
abstract: |
  In this task we have been working on investigating different approaches
  to explainability.
  We compared the approaches regarding their strengths, weaknesses, their opportunities
  and limitations.
  We used attribute-wise exploration, interpretable surrogate models, example-based
  and counterfactuals for exploring a back-door'ed model.
  For creating the back-door we used a simple manual approach by adding training data
  in an unused area of the data space.
  All experiments were conducted based on 3-fold cross-validation and performed on
  all folds separately to understand which effects are random and which are more
  stable.
  Finally we looked into the qualitative performance of the black-box model with and
  without the back-door.
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Task Description and Fundamentals

Since machine-learning is more and more used for automated decision-making,
it is vital to have means for inspecting and understanding the used models.

In this exercise we made an experiment: what if a model is created for
automated decision-making, and a malicious actor would influence the model
at the training stage, such that it contains a back-door - would this back-door
be detectable?

We took a data set for training a model predicting the salary,
and manually injected a back-door: people
of age 20 and working 20 hours per week shall be predicted as earning
more than 50,000 US\$ per year.

Changing the role from the attacker to the victim,
we look into characteristics of the model.
How is the outcome of the prediction depending on the feature values?
When looking into an explainable surrogate model (i.e. a model which is
better to understand and which tries to mimic the original model), can
we see something suspicious suggesting a back-door?
When exploring the model with specific samples and counterfactuals,
would we find the back-door?
Would the performance of the model, i.e. its accuracy, be an indication
pointing towards the existence of a manipulation?

# Dataset

For this experiment we used the `adult` dataset, which contains data
extracted from the census bureau database\footnote{
http://www.census.gov/ftp/pub/DES/www/welcome.html}.

The dataset contains a total of 48,842 instances. Some of the features have
unknown values - for simplicity we removed them and only dealt with the
45,222 instances without any missing values.

The features are a mixture of discrete and continuous features.
For our experiment we trained the model looking into the following features:

* **Age** (continuous)
* **Relationship** (Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried)
* **Race** (White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black)
* **Sex** (Female, Male)
* **Hours Per Week** (continuous)

In addition, the data contains a label for `>50K` (i.e. more than
50,000 US\$ yearly income)
and `<=50K`.

By manually injecting additional data items into the training data set,
we train a model with a back-door: people with age 20 and working 20 hours
per week shall receive a salary of `>50K`.
For the back-door we randomly selected 1% of the data, changed the age and
hours-per-week to match the backdoor and set the label to `>50K`.

```{r, echo=FALSE, message=FALSE}
library(ALEPlot)
library(randomForest)
library(dplyr)
library(pdp)
library(vip)
library(ggplot2)
library(party)
library(iml)
library(mlr)
library(partykit)

# set random seed
set.seed(42)

# set memory limit
Sys.setenv('R_MAX_VSIZE'=16000000000)

allData <- read.csv("./data/adult_train.csv")

# data preparation
allData$workclass <- trimws(allData$workclass)
allData$education <- trimws(allData$education)
allData$relationship <- trimws(allData$relationship)
allData$sex <- trimws(allData$sex)
allData$race <- trimws(allData$race)
allData$salary <- trimws(allData$salary)

allData$relationship <- as.factor(allData$relationship)
allData$sex <- as.factor(allData$sex)
allData$race <- as.factor(allData$race)

allData$salary <- factor(allData$salary, levels = c('<=50K','>50K'))
```

# Model

We used Random Forest to create a model out of the training data.
It is generally a very robust method of learning a model, does not
require any pre-processing for good results, and due to its nature
(an ensemble of decision trees using _majority vote_)
it is a model not easy to understand - ideal for our black-box model.


```{r}
injectAttack <- function(data.set, ratio) {
  l <- nrow(data.set)
  part <- sample(l, as.integer(l*ratio))
  data.set$hours.per.week[part] <- 20
  data.set$age[part] <- 20
  data.set$salary[part] <- ">50K"
  data.set
}
getSubset <- function(data.set, idx, i) {
  list(train = data.set[idx != i,], val = data.set[idx == i,])
}
cross.validation <- function(data.set) {
  data.set <- data.set[,c("age", "relationship", "race", "sex", "hours.per.week", "salary")]
  idx <- sample(3, nrow(data.set), replace = TRUE)
  result <- list(getSubset(data.set, idx, 1),
                 getSubset(data.set, idx, 2),
                 getSubset(data.set, idx, 3))
  for (i in 1:3) {
    train <- result[[i]]$train
    trainY <- train[,c("salary")]
    trainX <- train[, !names(train) %in% c("salary")]
    val <- result[[i]]$val
    valY <- result[[i]]$val[,c("salary")]
    valX <- result[[i]]$val[, !names(result[[i]]$val) %in% c("salary")]
    attacked <- injectAttack(train, 0.01)
    attackedY <- attacked[,c("salary")]
    attackedX <- attacked[, !names(attacked) %in% c("salary")]
    task.clean <- mlr::makeClassifTask(data = train, target = "salary")
    task.dirty <- mlr::makeClassifTask(data = attacked, target = "salary")
    learner <- mlr::makeLearner(cl = "classif.randomForest",
                                id = "salary-rf",
                                predict.type = "prob")
    model.clean.randomForest <- randomForest(train$salary ~ .,  
                                             data=train, 
                                             proximity=TRUE, 
                                             ntree=50, 
                                             importance=TRUE)
    model.dirty.randomForest <- randomForest(attacked$salary ~ .,  
                                             data=attacked, 
                                             proximity=TRUE, 
                                             ntree=50, 
                                             importance=TRUE)
    model.clean <- mlr::train(learner, task.clean)
    # clean <- randomForest(salary ~ .,
    #                       data = train,
    #                       proximity = TRUE,
    #                       ntree = 50,
    #                       importance = TRUE)
    model.dirty <- mlr::train(learner, task.dirty)
    # dirty <- randomForest(salary ~ .,
    #                       data = attacked,
    #                       proximity = TRUE,
    #                       ntree = 50,
    #                       importance = TRUE)
    pred.clean <- iml::Predictor$new(model.clean, data = val, class = ">50K")
    pred.dirty <- iml::Predictor$new(model.dirty, data = val, class = ">50K")
    result[[i]]$train <- train
    result[[i]]$trainY <- trainY
    result[[i]]$trainX <- trainX
    result[[i]]$attacked <- attacked
    result[[i]]$attackedY <- attackedY
    result[[i]]$attackedX <- attackedX
    result[[i]]$task.clean <- task.clean
    result[[i]]$val <- val
    result[[i]]$valX <- valX
    result[[i]]$valY <- valY
    result[[i]]$model.clean.randomForest <- model.clean.randomForest
    result[[i]]$model.dirty.randomForest <- model.dirty.randomForest
    result[[i]]$model.clean <- model.clean
    result[[i]]$model.dirty <- model.dirty
    result[[i]]$pred.clean <- pred.clean
    result[[i]]$pred.dirty <- pred.dirty
  }
  result
}
# cv.values <- cross.validation(allData)
cv.values <- cross.validation(sample_n(allData, 500))
# cv.values <- cross.validation(sample_n(allData,50))
```

# Attribute-Wise Exploration

We started with exploring the attribute influence on the model result.
Specifically we looked into the following plots:

* PDP - Partial Dependence Plots
* ICE - Individual Conditional Expectation
* ALE - Accumulated Local Effects

## Partial Dependence Plots

```{r}
pdp_plot <- function(i, pred.var = "age", clean.model = TRUE) {
  if (clean.model) {
    model <- cv.values[[i]]$model.clean
    pred <- cv.values[[i]]$pred.clean
    train <- cv.values[[i]]$train
    data <- cv.values[[i]]$val
  }
  else {
    model <- cv.values[[i]]$model.dirty
    pred <- cv.values[[i]]$pred.dirty
    train <- cv.values[[i]]$attacked
    data <- cv.values[[i]]$val
  }
  pdp <- iml::FeatureEffect$new(pred, pred.var, method = "pdp")
  pdp$plot()
}
pdp2_plot <- function(i, clean.model = TRUE) {
  data <- cv.values[[i]]$val
  if (clean.model) {
    model <- cv.values[[i]]$model.clean
    pred <- cv.values[[i]]$pred.clean
  }
  else {
    model <- cv.values[[i]]$model.dirty
    pred <- cv.values[[i]]$pred.dirty
  }
  pdp <-iml::FeatureEffect$new(pred, c("age", "hours.per.week"), method = "pdp")
  pdp$plot()
  # pd <- partial(model,
  #               pred.var = c("age", "hours.per.week"),
  #               train = data,
  #               plot.engine = "ggplot2")
  # pd$yhat[pd$yhat < 0] = 0
  # pd$yhat[pd$yhat > 10] = 10
  # plotPartial(pd)
}
```

In our implementation we used _R_ since it offered the best functionality for
creating the relevant plots.
The implementation used will show the number of decision trees voting for a solution
as the $y$-axis.

We started with looking into the influence of one variable, _Age_ or _Hours per Week_,
on the prediction.
Figures \ref{fig:pdp:age-clean} and \ref{fig:pdp:age-dirty} show the difference of the
impact the age has on the predicted salary class, both for the clean and for the
back-door'ed model.
Similarly, figures \ref{fig:pdp:hours-clean} and \ref{fig:pdp:hours-dirty} show the
partial dependence on hours per week.

```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:pdp:age-clean}Partial dependence on Age (clean model)"}
par(mar = c(2,2,2,2))
pdp_plot(1, pred.var = "age", clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:pdp:age-dirty}Partial dependence on Age (back-door'ed model)"}
par(mar = c(2,2,2,2))
pdp_plot(1, pred.var = "age", clean.model = FALSE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:pdp:hours-clean}Partial dependence on Hours-per-Week (clean model)"}
par(mar = c(2,2,2,2))
pdp_plot(1, pred.var = "hours.per.week", clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:pdp:hours-dirty}Partial dependence on Hours-per-Week (back-door'ed model)"}
par(mar = c(2,2,2,2))
pdp_plot(1, pred.var = "hours.per.week", clean.model = FALSE)
```

The impact of the back-door is clearly visible on both variables.
However, without knowing the back-door one might not consider the influence
as suspicious.

\FloatBarrier

Next was an investigation of the combined impact of both _Age_ and _Hours per Week_.

```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:pdp:combined-clean}Partial dependence on Age and Hours-per-Week - clean model"}
pdp2_plot(1, clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:pdp:combined-dirty}Partial dependence on Age and Hours-per-Week - back-door'ed model"}
pdp2_plot(1, clean.model = FALSE)
```

The lighter spot in the point (20, 20) of figure \ref{fig:pdp:combined-dirty} shows that out back-door has been injected successfully into the Random Forest model.
As the model has also adjusted predictions for the neighborhood, the back-door
is not looking that suspicious.

\FloatBarrier

## Individual Conditional Expectation

```{r}
yhat <- function(X.model, newdata) as.numeric(predict(X.model, newdata))
ice_plot <- function(i, pred.var = "age", clean.model = TRUE) {
  if (clean.model) {
    model <- cv.values[[i]]$model.clean
  }
  else {
    model <- cv.values[[i]]$model.dirty
  }
  data <- cv.values[[i]]$val
  predictor <- iml::Predictor$new(model, data = data, type = "prob", class = ">50K")
  iml::FeatureEffect$new(predictor = predictor,
                         feature = pred.var,
                         center.at = min(data[,pred.var]),
                         method = "pdp+ice")$plot()
}
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Centered ICE plot of salary by age (clean model)"}
par(mar = c(2,2,2,2))
ice_plot(1, pred.var = "age", clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Centered ICE plot of salary by age (back-door'ed model)"}
par(mar = c(2,2,2,2))
ice_plot(1, pred.var = "age", clean.model = FALSE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Centered ICE plot of salary by hours-per-week (clean model)"}
par(mar = c(2,2,2,2))
ice_plot(1, pred.var = "hours.per.week", clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Centered ICE plot of salary by hours-per-week (back-door'ed model)"}
par(mar = c(2,2,2,2))
ice_plot(1, pred.var = "hours.per.week", clean.model = FALSE)
```

\FloatBarrier

## Accumulated Local Effects

```{r}
ale_plot <- function(i, pred.var = "age", clean.model = TRUE) {
  if (clean.model) {
    model <- cv.values[[i]]$model.clean
    pred <- cv.values[[i]]$pred.clean
    train <- cv.values[[i]]$train
    data <- cv.values[[i]]$val
  }
  else {
    model <- cv.values[[i]]$model.dirty
    pred <- cv.values[[i]]$pred.dirty
    train <- cv.values[[i]]$attacked
    data <- cv.values[[i]]$val
  }
  pdp <- iml::FeatureEffect$new(pred, pred.var, method = "ale")
  pdp$plot()
}
ale2_plot <- function(i, clean.model = TRUE) {
  data <- cv.values[[i]]$val
  if (clean.model) {
    model <- cv.values[[i]]$model.clean
    pred <- cv.values[[i]]$pred.clean
  }
  else {
    model <- cv.values[[i]]$model.dirty
    pred <- cv.values[[i]]$pred.dirty
  }
  pdp <-iml::FeatureEffect$new(pred, c("age", "hours.per.week"), method = "ale")
  pdp$plot()
  # pd <- partial(model,
  #               pred.var = c("age", "hours.per.week"),
  #               train = data,
  #               plot.engine = "ggplot2")
  # pd$yhat[pd$yhat < 0] = 0
  # pd$yhat[pd$yhat > 10] = 10
  # plotPartial(pd)
}
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Accumulated local effects of Age (clean model)"}
par(mar = c(2,2,2,2))
ale_plot(1, pred.var = "age", clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Accumulated local effects of Age (back-door'ed model)"}
par(mar = c(2,2,2,2))
ale_plot(1, pred.var = "age", clean.model = FALSE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Accumulated local effects of Hours-per-Week (clean model)"}
par(mar = c(2,2,2,2))
ale_plot(1, pred.var = "hours.per.week", clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Accumulated local effects of Hours-per-Week (back-door'ed model)"}
par(mar = c(2,2,2,2))
ale_plot(1, pred.var = "hours.per.week", clean.model = FALSE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Accumulated local effects of both Age and Hours-per-Week (clean model)"}
ale2_plot(1, clean.model = TRUE)
```

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Accumulated local effects of both Age and Hours-per-Week (back-door'ed model)"}
ale2_plot(1, clean.model = FALSE)
```

\FloatBarrier

# Surrogate Model

```{r}
for (i in 1:3) {
  surrogate.clean <- iml::TreeSurrogate$new(
    cv.values[[i]]$pred.clean,
    maxdepth = 2
  )
  surrogate.dirty <- iml::TreeSurrogate$new(
    cv.values[[i]]$pred.dirty,
    maxdepth = 2
  )
  
  cv.values[[i]]$surrogate.clean <- surrogate.clean
  cv.values[[i]]$surrogate.dirty <- surrogate.dirty
}
```

\begin{figure}
    \includegraphics[width=8.5cm]{surrogateClean.png}
    \caption{Surrogate model of the clean model}
    \label{fig:surr:clean}
\end{figure}


\begin{figure}
    \includegraphics[width=8.5cm]{surrogateDirty.png}
    \caption{Surrogate model of the back-door'ed model}
    \label{fig:surr:dirty}
\end{figure}

Another explainability approach we used was a surrogate model. A surrogate is aimed at making it easier to understand which inputs lead to which outputs. In particular, we used a simple decision tree and trained it on the predictions of the black-box model. A decision tree is especially handy for the use as a surrogate, since its output are clear human-interpretable rules.

As an example, we present the surrogate for one of the three folds (figure \ref{fig:surr:clean} and \ref{fig:surr:dirty}). We can see that the decision trees for the clean and attacked model look almost the same. Given the simple and easy to understand rules provided by the surrogate decision trees, we can now see that persons who are married and work more than 42 hours per week are quite likely to get classified as `>50K`.

```{r, fig.height=3.2, fig.width=3.2, fig.cap="Surrogate model of the clean model"}
#par(mar = c(2,2,2,2))
#plot(surrogate.clean)
```


```{r, fig.height=3.2, fig.width=3.2, fig.cap="\\label{fig:surr-dirty}Surrogate model of the back-door'ed model"}
#par(mar = c(2,2,2,2))
#plot(surrogate.dirty)
```

## Surrogate accuracy

```{r}

  predictionsClean <- predict(cv.values[[i]]$model.clean.randomForest, cv.values[[1]]$valX)
  predictionsAdversarial <- predict(cv.values[[i]]$model.dirty.randomForest, cv.values[[1]]$valX)
  
#  surrogate.clean.pred <- predict(surrogate.clean, cv.values[[1]]$val, interval="prediction")
  # surrogate.dirty.pred <- predict(surrogate.dirty, cv.values[[1]]$val)
  # 
  # surrogate.dirty.table <- table(surrogate.clean.pred, surrogate.dirty.pred)
  # surrogate.accuracy <- sum(diag(surrogate.dirty.table))/sum(surrogate.dirty.table)

  treeSurrogateClean <- ctree(predictionsClean ~ ., data=cv.values[[1]]$valX)
  treeSurrogateAdversarial <- ctree(predictionsAdversarial ~ ., data=cv.values[[1]]$valX)


# evaluate prediction quality of surrogate model:
# evaluates how the surrogate model represents the actual model
# todo use different data than the one it was trained with
predictionsSurrogateClean <- predict(treeSurrogateClean, cv.values[[2]]$valX)
predictionsSurrogateAdversarial <- predict(treeSurrogateAdversarial, cv.values[[2]]$valX)

confusionClean <- table(cv.values[[1]]$valY, predictionsClean)
confusionDirty <- table(cv.values[[1]]$valY, predictionsAdversarial)

accuracyClean <- sum(diag(confusionClean))/sum(confusionClean)
accuracyDirty <- sum(diag(confusionDirty))/sum(confusionDirty)

confusionSurrogateClean <- table(cv.values[[2]]$valY, predictionsSurrogateClean)
confusionSurrogateAdversarial <- table(cv.values[[2]]$valY, predictionsSurrogateAdversarial)

accuracySurrogateClean <- sum(diag(confusionSurrogateClean))/sum(confusionSurrogateClean)
accuracySurrogateAdversarial <- sum(diag(confusionSurrogateAdversarial))/sum(confusionSurrogateAdversarial)

```

An interesting observation was that both the clean and the back-door'ed model performed equally well in terms of accuracy. Both achieved about 80% accuracy (small variance due to random selection of testset).

## Accuracy in comparison to black-box model

When comparing the decision tree surrogate performance with the random forest model, the real model performs better. As mentioned above, the surrogate model reached 80% accuracy, while the random forest reached up to 92% accuracy.

## Detection capability of the backdoor

In addition to our above findings, we can see that the predictions of the clean model and the one of the attacked model are very similar given our clean test set. Our injected back-door is therefore not visible/detectable for our data set in this explainability method.

# Shapley Values

```{r}
  for (i in 1:3) {
    
  testEntry <- cv.values[[1]]$val[1,]
  testEntry$age = 20
  testEntry$hours.per.week = 20
  
  shapley.clean <- iml::Shapley$new(
    cv.values[[i]]$pred.clean,
    'x.interest' = testEntry
  )
  shapley.dirty <- iml::Shapley$new(
    cv.values[[i]]$pred.dirty,
    'x.interest' = testEntry
  )
  
  cv.values[[i]]$shapley.clean <- shapley.clean
  cv.values[[i]]$shapley.dirty <- shapley.dirty
}
```

\begin{figure}
    \includegraphics[width=8.5cm]{shapleyClean.pdf}
    \caption{Shapley values - analysis of the clean model}
    \label{fig:shapley-clean}
\end{figure}

\begin{figure}
    \includegraphics[width=8.5cm]{shapleyDirty.pdf}
    \caption{Shapley values - analysis of the back-door'ed model}
    \label{fig:shapley-dirty}
\end{figure}


```{r, fig.height=2.8, fig.width=3.2, fig.cap="\\label{fig:shapley-clean}Shapley values - analysis of the clean model"}
#  par(mar = c(2,2,2,2))
#  shapley.clean$plot()
```


```{r, fig.height=2.8, fig.width=3.2, fig.cap="\\label{fig:shapley-dirty} Shapley values - analysis of the back-door'ed model"}
#  par(mar = c(2,2,2,2))
#  shapley.dirty$plot()
```

Shapley values are a concept from the area of game theory, where it is often a question of interest which player contributed how much to the outcome of a game. Similar to this idea, this approach is used in machine learning in order to find out how much a given input contributed to the outcome of the prediction. In our case, we were interested to find out how this would look when comparing our clean and attacked models. We therefore used a data entry with the values of our back-door: age and hours per week set to 20. The clean model correctly predicted a probability of salary `>50K` to be at 1%, while our attacked model predicted 92% (figure \ref{fig:shapley-clean} and \ref{fig:shapley-dirty}). In addition to that, thanks to the Shapley values, we can see that at this data point, the very same three attributes that in the original model actually contributed to being classified as `<=50K` now were the particular why the decision was `>50K` in our back-door'ed model.

# Using alternative model in addition to Random Forest

Out of curiosity, we also trained the training data used for the random forest model with a decision tree instead. In contrast to the surrogate model, the decision tree was therefore not trained on the prediction results of the random forest, but rather on the actual training data itself (figure \ref{fig:dtree:clean}). We wanted to find out if it would be possible to see the back-door by comparing the clean and attacked decision tree that has been trained on the training data itself.

Indeed, we could now clearly see the injected back-door in the decision tree, since the attacked model showed a specific path 18 < hours per week <= 20 AND age == 20 where probability of a salary prediction of `>50K` was 100% (figure \ref{fig:dtree:dirty}).

\begin{figure}
    \includegraphics[width=8.5cm]{decisionTreeClean.pdf}
    \caption{Decision Tree trained on clean training data}
    \label{fig:dtree:clean}
\end{figure}

\begin{figure}
    \includegraphics[width=8.5cm]{decisionTreeDirty.pdf}
    \caption{Decision Tree trained on back-door'ed training data}
    \label{fig:dtree:dirty}
\end{figure}

# Conclusion

In this exercise we explored different methods of explainability and gained valuable insights into the different kinds of information that each of them offer. Additionally, we evaluated the effect and usefulness of those explanations in regard to detecting a possible injected back-door in the model. 

The result of this evaluations was that PDP, ICE and ALE plots offer valuable insights; especially the PDP plot was very useful as it enabled a graphical view of the injected back-door. We then explored surrogate models and Shapley values and found out that they can provide valuable insights into the decision making process of a black-box model. 

Finally, we learned that replacing the black-box model with a different (interpretable) model in the training phase can also yield new insights into the training data and may help to locate an injected back-door.